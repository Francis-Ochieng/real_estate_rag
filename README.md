# Real Estate Assistant --- RAG Streamlit App (with FAISS + Groq)

## What this is

A Retrieval-Augmented Generation (RAG) Streamlit app tailored for real
estate workflows.\
It ingests PDFs, DOCX, TXT, CSV, webpages (scraping), and YouTube
transcripts, chunks text, creates embeddings, stores them locally in
**FAISS**, and lets you ask questions over your documents.

**Note:**\
- This version uses **Groq** for generation (instead of OpenAI).\
- The vector database is now **FAISS** (instead of Chroma).

------------------------------------------------------------------------

## âœ¨ Features

-   ğŸ“‚ Upload multiple files (PDF, TXT, DOCX, CSV)\
-   ğŸŒ Add website URLs and YouTube links (one per line)\
-   ğŸ§© Character-based chunking with adjustable size & overlap\
-   ğŸ” Embeddings with `sentence-transformers` (HuggingFace)\
-   ğŸ“¦ Vector store: **FAISS** (local, persistent)\
-   ğŸ¤– Generation with **Groq LLMs** (`llama-3.1-*`)\
-   ğŸ–¥ï¸ Streamlit UI: chat history, clear chats, preview docs, download
    transcript\
-   ğŸ“Š Shows retrieved passages & similarity scores

------------------------------------------------------------------------

## ğŸ“‚ Folder layout

    real_estate_rag/
    â”œâ”€ app.py                # Streamlit app (main)
    â”œâ”€ ingestion.py          # Loaders for files, URLs, YouTube
    â”œâ”€ embeddings.py         # Embedding model wrapper (Groq/HF)
    â”œâ”€ retriever.py          # FAISS-based retriever
    â”œâ”€ utils.py              # Helpers (chunking, cleaning)
    â”œâ”€ requirements.txt
    â””â”€ README.md

------------------------------------------------------------------------

## âš¡ Quick setup

Create a Python 3.11+ virtual environment:

``` bash
python -m venv .venv
source .venv/bin/activate   # or .\.venv\Scripts\activate on Windows
pip install -r requirements.txt
```

Set your **Groq API key** in a `.env` file:

    GROQ_API_KEY=your_groq_api_key_here

Run the Streamlit app:

``` bash
streamlit run app.py
```

Use the sidebar to upload files, paste URLs/YouTube links, configure
chunking, then click **Ingest**. After ingestion you can ask questions
in the chat box.

------------------------------------------------------------------------

## ğŸš€ Notes & Limitations

-   The app uses `sentence-transformers/all-MiniLM-L6-v2` locally for
    embeddings.\
    (Model weights download on first run.)\
-   Groq provides **fast inference** for LLaMA 3 models. Without a valid
    API key, generation is disabled.\
-   FAISS index is stored locally (`faiss_index/`) and persists between
    runs.\
-   Reranker option is present but **not yet supported** with Groq.

------------------------------------------------------------------------

## ğŸ” Example queries

-   "Which properties in the document are within 2 km of the CBD?"\
-   "Summarize the rental trends mentioned in the uploaded reports."\
-   "List the key amenities for Apartment X and its monthly rent."

------------------------------------------------------------------------

## ğŸ› ï¸ Future improvements

-   Add authentication before ingesting private docs\
-   Enable hybrid search (BM25 + FAISS)\
-   Long-document summarization & caching of embeddings\
-   Deploy to Streamlit Cloud or Render for a live demo

------------------------------------------------------------------------
